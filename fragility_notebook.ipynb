{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b55a6f",
   "metadata": {},
   "source": [
    "# Notebook for first run through of fragility code\n",
    "1. All .py files sent will be included in first few cells \n",
    "2. The actual example files will use edfs from intracranial data I am working with (#example-code)\n",
    "\n",
    "All code on Python 3.12.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87081e44",
   "metadata": {},
   "source": [
    "## Fragility code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab8418",
   "metadata": {},
   "source": [
    "Sysid file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6270946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.sparse import linalg\n",
    "\n",
    "SUPPORTED_HANKEL_VECTORIZATIONS = [\"coo\", \"dok\", \"csr\"]\n",
    "\n",
    "\n",
    "class SIDBase(object):\n",
    "    \"\"\"System Identification base class.\n",
    "\n",
    "    Helper functions for linear system identification and common\n",
    "    statistical methods for de-biasing the estimation. For example:\n",
    "\n",
    "    1. Forward-backwards\n",
    "    2. Total least-squares\n",
    "    \"\"\"\n",
    "\n",
    "    def _construct_snapshots(self, snapshots, order, n_times):\n",
    "        snaps = np.concatenate(\n",
    "            [snapshots[:, i : n_times - order + i + 1] for i in range(order)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return snaps\n",
    "\n",
    "    @staticmethod\n",
    "    def _col_major_2darray(X):\n",
    "        \"\"\"Store snapshots into a 2D matrix, by column.\n",
    "\n",
    "        If the input data is already formatted as 2D\n",
    "        array, the method saves it, otherwise it also saves the original\n",
    "        snapshots shape and reshapes the snapshots.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : int or numpy.ndarray\n",
    "            the input snapshots.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        snapshots : np.ndarray\n",
    "            the 2D matrix that contains the flatten snapshots\n",
    "        snapshots_shape : tuple\n",
    "            the shape of original snapshots\n",
    "        \"\"\"\n",
    "        # If the data is already 2D ndarray\n",
    "        if isinstance(X, np.ndarray) and X.ndim == 2:\n",
    "            snapshots = X\n",
    "            snapshots_shape = None\n",
    "        else:\n",
    "            input_shapes = [np.asarray(x).shape for x in X]\n",
    "\n",
    "            if len(set(input_shapes)) != 1:\n",
    "                raise ValueError(\"Snapshots have not the same dimension.\")\n",
    "\n",
    "            snapshots_shape = input_shapes[0]\n",
    "            snapshots = np.transpose([np.asarray(x).flatten() for x in X])\n",
    "\n",
    "\n",
    "        return snapshots, snapshots_shape\n",
    "\n",
    "    def _solve_regression(\n",
    "        self,\n",
    "        X,\n",
    "        Y,\n",
    "        l2_penalty,\n",
    "        solver=\"auto\",\n",
    "        fit_intercept=False,\n",
    "        normalize=False,\n",
    "        random_state=123456,\n",
    "        sample_weight=None,\n",
    "        ch_weight=None,\n",
    "    ):\n",
    "        from sklearn.linear_model import Ridge\n",
    "\n",
    "        clf = Ridge(\n",
    "            alpha=l2_penalty,\n",
    "            fit_intercept=fit_intercept,\n",
    "            normalize=normalize,\n",
    "            solver=solver,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        # n_samples X n_features and n_samples X n_targets\n",
    "        clf.fit(X.T, Y.T, sample_weight=sample_weight)\n",
    "\n",
    "        # n_targets X n_features\n",
    "        A = clf.coef_\n",
    "        return A\n",
    "\n",
    "    def _pinv_solve(self, X, Y, l2_penalty, solver=\"svd\", backend=\"numpy\"):\n",
    "        \"\"\"Solves linear regression using pseudoinverse.\n",
    "\n",
    "        Can either use standard pseudoinverse solve, or\n",
    "        performs l2-regularization [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            the first matrix;\n",
    "        Y : np.ndarray\n",
    "            the second matrix;\n",
    "        l2_penalty : float | None\n",
    "            The l2 penalty coefficient in L2 regularization.\n",
    "        solver : str\n",
    "            The solver strategy for linear regression. Either uses\n",
    "            ``'lstsq'`` (calls :func:`np.linalg.pinv`, or :func:`np.linalg.lstsq`),\n",
    "             or ``'svd'`` (uses SVD methods).\n",
    "        backend : str\n",
    "            Either ``'numpy'`` (default), or ``'scipy'``.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        adjmat : np.ndarray\n",
    "            The corresponding ``A`` matrix.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] https://www.cs.princeton.edu/courses/archive/spring07/cos424/scribe_notes/0403.pdf  # noqa\n",
    "        \"\"\"\n",
    "        if backend not in [\"scipy\", \"numpy\"]:\n",
    "            msg = f'Only \"scipy\", or \"numpy\" backend is accepted, not {backend}.'\n",
    "            raise ValueError(msg)\n",
    "        elif backend == \"scipy\":\n",
    "            backend_func = scipy\n",
    "        elif backend == \"numpy\":\n",
    "            backend_func = np\n",
    "\n",
    "        n_chs = X.shape[0]\n",
    "\n",
    "        if l2_penalty is None or l2_penalty == 0:\n",
    "            if solver == \"lstsq\":\n",
    "                adjmat = Y.dot(scipy.linalg.pinv(X))\n",
    "            elif solver == \"svd\":\n",
    "                # or use numpy linalg\n",
    "                adjmat = Y.dot(backend_func.linalg.pinv(X))\n",
    "        else:  # pragma: no cover\n",
    "\n",
    "            # apply perturbation on diagonal of X^TX\n",
    "            tikhonov_regularization = l2_penalty * np.eye(n_chs)\n",
    "\n",
    "            if solver == \"lstsq\":\n",
    "                adjmat = backend_func.linalg.solve(\n",
    "                    X.T.dot(X) + tikhonov_regularization, X.T.dot(Y)\n",
    "                )\n",
    "            elif solver == \"svd\":\n",
    "                inner_regularized = X.dot(X.T) + tikhonov_regularization\n",
    "                _tikhonov_pinv_mat = X.T.dot(np.linalg.inv(inner_regularized))\n",
    "\n",
    "                # solve analytical solution for regularized L2 regression\n",
    "                adjmat = Y.dot(_tikhonov_pinv_mat)\n",
    "        return adjmat\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_tlsq(X, Y, tlsq_rank):\n",
    "        \"\"\"Compute Total Least Square.\n",
    "\n",
    "        Assumes the model is the following:\n",
    "        ::\n",
    "\n",
    "            Y = AX\n",
    "\n",
    "        where ``X`` and ``Y`` are passed in arrays.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            the first matrix;\n",
    "        Y : np.ndarray\n",
    "            the second matrix;\n",
    "        tlsq_rank : int\n",
    "            the rank for the truncation; If 0, the method\n",
    "            does not compute any noise reduction; if positive number, the\n",
    "            method uses the argument for the SVD truncation used in the TLSQ\n",
    "            method.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : np.ndarray\n",
    "            the denoised matrix X,\n",
    "        Y : np.ndarray\n",
    "            the denoised matrix Y\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] https://arxiv.org/pdf/1703.11004.pdf\n",
    "        .. [2] https://arxiv.org/pdf/1502.03854.pdf\n",
    "        \"\"\"\n",
    "        # Do not perform tlsq\n",
    "        if tlsq_rank == 0:\n",
    "            return X, Y\n",
    "\n",
    "        V = np.linalg.svd(np.append(X, Y, axis=0), full_matrices=False)[-1]\n",
    "        rank = min(tlsq_rank, V.shape[0])\n",
    "        VV = V[:rank, :].conj().T.dot(V[:rank, :])\n",
    "\n",
    "        return X.dot(VV), Y.dot(VV)\n",
    "\n",
    "\n",
    "class PostHocMixin:\n",
    "    \"\"\"Mixin class for performing post-hoc modifications to identified systems.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def stabilize_matrix(A, eigval):\n",
    "        \"\"\"\n",
    "        Stabilize the matrix, A based on its eigenvalues.\n",
    "\n",
    "        Assumes discrete-time linear system.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : np.ndarray\n",
    "             CxC matrix\n",
    "        eigval : float\n",
    "            the maximum eigenvalue to shift all large evals to\n",
    "        Returns\n",
    "        -------\n",
    "        A : np.ndarray\n",
    "            the stabilized matrix w/ eigenvalues <= eigval\n",
    "        \"\"\"\n",
    "        # check if there are unstable eigenvalues for this matrix\n",
    "        if np.sum(np.abs(np.linalg.eigvals(A)) > eigval) > 0:\n",
    "            # perform eigenvalue decomposition\n",
    "            eigA, V = scipy.linalg.eig(A)\n",
    "\n",
    "            # get magnitude of these eigenvalues\n",
    "            abs_eigA = np.abs(eigA)\n",
    "\n",
    "            # get the indcies of the unstable eigenvalues\n",
    "            unstable_inds = np.where(abs_eigA > eigval)[0]\n",
    "\n",
    "            # move all unstable evals to magnitude eigval\n",
    "            for ind in unstable_inds:\n",
    "                # extract the unstable eval and magnitude\n",
    "                unstable_eig = eigA[ind]\n",
    "                this_abs_eig = abs_eigA[ind]\n",
    "\n",
    "                # compute scaling factor and rescale eval\n",
    "                k = eigval / this_abs_eig\n",
    "                eigA[ind] = k * unstable_eig\n",
    "\n",
    "            # recompute A\n",
    "            eigA_mat = np.diag(eigA)\n",
    "            Aprime = np.dot(V, eigA_mat)\n",
    "            A = scipy.linalg.lstsq(V.T, Aprime.T)[0].T\n",
    "        return A\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab644f90",
   "metadata": {},
   "source": [
    "Perturbation model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502670ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import utils.dataconfig as constants\n",
    "\n",
    "SUPPORTED_PERTURBATION_OPT_METHODS = [\"dc\", \"grid\"]\n",
    "PERTURBATION_STRATEGIES = [\"univariate\", \"bivariate\"]\n",
    "\n",
    "\n",
    "def ensure_list(arg):\n",
    "    if not (isinstance(arg, list)):\n",
    "        try:  # if iterable\n",
    "            if isinstance(arg, (str, dict)):\n",
    "                arg = [arg]\n",
    "            elif arg is None:\n",
    "                arg = []\n",
    "            else:\n",
    "                arg = list(arg)\n",
    "        except BaseException:  # if not iterable\n",
    "            arg = [arg]\n",
    "    return arg\n",
    "\n",
    "\n",
    "\n",
    "def _create_standard_basis_vector(size, index, as_vector=False):\n",
    "    arr = np.zeros(size)\n",
    "    arr[index] = 1.0\n",
    "    if as_vector:  # turn into 1 vector in R^d, not a list\n",
    "        arr = arr[:, np.newaxis]\n",
    "    return arr\n",
    "\n",
    "\n",
    "def compute_brauer_rank_one(\n",
    "    A: np.ndarray, radius: complex, eigval_idx: int = 0, rank: int = 1\n",
    "):\n",
    "    \"\"\"Applies Brauer's rank one perturbation algorithm.\n",
    "\n",
    "    Perturbs the leading eigenvalue of matrix A to obtain eigenvalue\n",
    "    with value at ``radius``. Will perform a perturbation on the\n",
    "    eigenvalue at index ``eigval_idx``, which is the index after\n",
    "    sorting the eigenvalues from largest to smallest in absolute value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    %(A)s\n",
    "    %(radius)s\n",
    "    eigval_idx : int\n",
    "        The eigenvalue to apply perturbation to.\n",
    "    rank : int\n",
    "        The rank of the perturbation to apply. (Default = 1).\n",
    "        Results in ``rank`` number of eigenvalues that are\n",
    "        changed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    delta_vec : np.ndarray\n",
    "        The (N, N) rank-1 matrix that when applied to ``A`` will\n",
    "        result in an eigenvalue with value ``radius``.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] A. Brauer, Limits for the characteristic roots of a matrix IV: Applications to stochastic matrices, Duke Math. J. 19\n",
    "        (1952) 75–91.\n",
    "    .. [2] H. Perfect, Methods of constructing certain stochastic matrices II, Duke Math. J. 22 (1955) 305–311.\n",
    "    \"\"\"\n",
    "    # perform eigenvalue decomposition of A\n",
    "    W, V = scipy.linalg.eig(A)\n",
    "\n",
    "    # get max to min of eigenvalues\n",
    "    sorted_inds = np.argsort(W)[::-1]\n",
    "\n",
    "    # take eigenvalue as the first entry\n",
    "    eigval = W[sorted_inds[eigval_idx]]\n",
    "    eigvec = V[:, sorted_inds[eigval_idx]]\n",
    "\n",
    "    # compute the perturbation of lambda\n",
    "    eigval_diff = radius - eigval\n",
    "\n",
    "    # format into 2D arrays to perform least squares\n",
    "    eigvec_arr = eigvec[None, :]\n",
    "    eigval_diff_arr = np.array([radius - eigval])\n",
    "\n",
    "    # solve least squares sense\n",
    "    eigval_perturbation_vec, res, rnk, s = scipy.linalg.lstsq(\n",
    "        eigvec_arr, eigval_diff_arr\n",
    "    )\n",
    "    eigval_perturbation = eigval_perturbation_vec.conj().T.dot(eigvec)\n",
    "    assert np.allclose(eigval_diff, eigval_perturbation)\n",
    "\n",
    "    # compute the delta vector\n",
    "    delta_vec = np.outer(eigvec, eigval_perturbation_vec.conj().T)\n",
    "\n",
    "    return delta_vec\n",
    "\n",
    "\n",
    "def compute_bauer_fike_bound(A, E, p=2):\n",
    "    \"\"\"Compute the Bauer-Fike bound.\n",
    "\n",
    "    Computes the bound of eigenvalue movement\n",
    "    based on the condition number of the\n",
    "    eigenvector matrix, and the p-norm of the\n",
    "    perturbation matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : np.ndarray\n",
    "        The matrix. Must be diagonalizable.\n",
    "    E : np.ndarray\n",
    "    p : str | int\n",
    "        The p-norm. Corresponds to to the ``'p'``\n",
    "        in :func:`np.linalg.cond` and the\n",
    "        ``'ord'`` in :func:`np.linalg.norm`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bound : float\n",
    "        The Bauer-Fike theorem bound.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] https://en.wikipedia.org/wiki/Bauer%E2%80%93Fike_theorem\n",
    "    \"\"\"\n",
    "    # compute eigenvalue decomposition of A\n",
    "    W, V = scipy.linalg.eig(A)\n",
    "\n",
    "    # compute the condition number of V\n",
    "    cond_num = np.linalg.cond(V, p=p)\n",
    "\n",
    "    # compute the norm of the perturbation matrix\n",
    "    pert_norm = np.linalg.norm(E, ord=p)\n",
    "\n",
    "    return cond_num * pert_norm\n",
    "\n",
    "\n",
    "class StructuredPerturbationModel(object):\n",
    "    r\"\"\"Structured rank-1 perturbations of column/rows of a matrix.\n",
    "\n",
    "    :math:`x(t+1) = (A + {\\\\Delta}) x(t)`\n",
    "\n",
    "    The algorithm takes in a np.ndarray matrix of data that is NxN and computes the\n",
    "    minimum 2-norm required to have one eigenvalue of the matrix A at \"r\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    %(radius)s\n",
    "    %(perturb_type)s\n",
    "    %(perturbation_strategy)s\n",
    "    %(n_jobs)s\n",
    "    %(verbose)s\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Model can either perform grid search over all possible frequencies of perturbation (i.e. placements\n",
    "    along the circle specified by \"radius\". Application at only the DC frequency (a+jb = radius; b = 0)\n",
    "    seems to work sufficiently and is only one search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        radius: Union[float, str],\n",
    "        perturb_type: str,\n",
    "        perturbation_strategy: str = \"univariate\",\n",
    "        n_jobs: int = 1,\n",
    "        on_error: str = \"raise\",\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        # ensure perturbation type is capitalized\n",
    "        perturb_type = perturb_type.upper()\n",
    "\n",
    "        if perturb_type not in [\n",
    "            constants.PERTURBATIONTYPES.COLUMN_PERTURBATION.value,\n",
    "            constants.PERTURBATIONTYPES.ROW_PERTURBATION.value,\n",
    "        ]:\n",
    "            msg = (\n",
    "                \"Perturbation type can only be {}, or {} for now. You \"\n",
    "                \"passed in {}.\".format(\n",
    "                    constants.PERTURBATIONTYPES.COLUMN_PERTURBATION,\n",
    "                    constants.PERTURBATIONTYPES.ROW_PERTURBATION,\n",
    "                    perturb_type,\n",
    "                )\n",
    "            )\n",
    "            raise AttributeError(msg)\n",
    "        if perturbation_strategy not in PERTURBATION_STRATEGIES:\n",
    "            msg = (\n",
    "                f\"Only perturbation strategies {PERTURBATION_STRATEGIES} \"\n",
    "                f\"are accepted, not {perturbation_strategy}.\"\n",
    "            )\n",
    "            raise AttributeError(msg)\n",
    "\n",
    "        self._radius = radius\n",
    "        self._perturb_type = perturb_type\n",
    "        self.perturbation_strategy = perturbation_strategy\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        self.on_error = on_error\n",
    "\n",
    "        self._min_frequency = None\n",
    "        self._delta_vec_arr = None\n",
    "        self._min_norms = None\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Representation of model and its parameters.\"\"\"\n",
    "        return (\n",
    "            f\"Structured Perturbation Model | \"\n",
    "            f\"radius={self.radius}, perturb_type={self.perturb_type}\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def radius(self):\n",
    "        return self._radius\n",
    "\n",
    "    @property\n",
    "    def perturb_type(self):\n",
    "        return self._perturb_type\n",
    "\n",
    "    @property\n",
    "    def minimum_freqs(self) -> Union[List, int]:\n",
    "        \"\"\"Corresponding frequencies that minimum norm perturbations occur.\"\"\"\n",
    "        return self._min_frequency\n",
    "\n",
    "    @property\n",
    "    def minimum_delta_vectors(self) -> np.ndarray:\n",
    "        \"\"\"Corresponding perturbation vectors with minimum norm.\n",
    "\n",
    "        The delta vectors are N x N, where N is the number of channels.\n",
    "        The rows correspond to the perturbation vector. For example,\n",
    "        ``self._delta_vec_arr[0, :]``\n",
    "        \"\"\"\n",
    "        return self._delta_vec_arr\n",
    "\n",
    "    @property\n",
    "    def minimum_norms_vector(self):\n",
    "        \"\"\"Compute the l2 norms of the minimum-norm delta vectors.\n",
    "\n",
    "        This will result in a N x 1 vector.\n",
    "        \"\"\"\n",
    "        return np.linalg.norm(self.minimum_delta_vectors, ord=2, axis=1)\n",
    "\n",
    "    def _compute_min_norm_delta_vec(\n",
    "        self,\n",
    "        A: np.ndarray,\n",
    "        radius: complex,\n",
    "        eks: List[int],\n",
    "        orthogonal_constraints: List[int] = None,\n",
    "    ):\n",
    "        r\"\"\"Compute a $\\Delta$ matrix that has minimum 2-norm.\n",
    "\n",
    "        Based on the radius (``r = a + ib``, where ``a`` is the real\n",
    "        component and ``b`` is the imaginary component) on the complex plane,\n",
    "        a matrix, $\\Delta$ is computed such that $A + \\Delta$ has an\n",
    "        eigenvalue at ``radius``.\n",
    "\n",
    "        The $\\Delta$ matrix is a rank-1 matrix that comprises of the same\n",
    "        vector at the column/row indices in ``eks`` list.\n",
    "\n",
    "        Note that the $\\Delta$ matrix may be generalized to have multiple\n",
    "        non-zero columns, depending on the passed in indices ``eks``.\n",
    "        The perturbation is either a Row, or Column.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        %(A)s\n",
    "        radius : np.complex\n",
    "            The perturbation radius (``np.abs(radius)``), which may have a real and\n",
    "            imaginary component.\n",
    "        eks : List[int]\n",
    "            A list of the column/row indices to compute a perturbation on.\n",
    "        %(orthogonal_constraints)s\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        delta_vec : np.ndarray\n",
    "            The perturbation vector that is applied at rows/columns in ``eks``,\n",
    "            which as size ``(n_chs,)``.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To re-compute the $\\Delta$ matrix, one would add the ``delta_vec``\n",
    "        vector along the rows/columns of a (n_chs X n_chs) zero matrix.\n",
    "        For example: ..\n",
    "\n",
    "            delta_mat = np.zeros((n_chs, n_chs))\n",
    "\n",
    "            for edx in eks:\n",
    "                # if we used a row perturbation\n",
    "                delta_mat[edx, :] = delta_vec\n",
    "\n",
    "                # if we used a column perturbation\n",
    "                delta_mat[:, edx] = delta_vec\n",
    "\n",
    "            # now the perturbed A matrix should have\n",
    "            # eigenvalue at location radius\n",
    "            print(np.linalg.eigs(A + delta_mat))\n",
    "\n",
    "        \"\"\"\n",
    "        if orthogonal_constraints is None:\n",
    "            orthogonal_constraints = []\n",
    "        perturb_type = self._perturb_type\n",
    "\n",
    "        # ensure all indices of the unit vector are a list\n",
    "        eks = ensure_list(eks)\n",
    "\n",
    "        if isinstance(A, np.ndarray):\n",
    "            if A.shape[0] != A.shape[1]:\n",
    "                msg = (\n",
    "                    f\"State matrix must be a square matrix. The state matrix \"\n",
    "                    f\"passed in has shape {A.shape}.\"\n",
    "                )\n",
    "                raise AttributeError(msg)\n",
    "\n",
    "        # initialize function parameters\n",
    "        len_perturbation = A.shape[0]\n",
    "\n",
    "        # initialize vector to desired eigenvalue\n",
    "        sigma = radius.real\n",
    "        omega = radius.imag\n",
    "        desired_eig = sigma + 1j * omega\n",
    "\n",
    "        # generate list of the unit vectors\n",
    "        unit_vecs = [\n",
    "            _create_standard_basis_vector(len_perturbation, idx, as_vector=True)\n",
    "            for idx in eks\n",
    "        ]\n",
    "\n",
    "        # generate array of constraints\n",
    "        constraint_vec = np.array([0, -1])\n",
    "        bvec = np.tile(constraint_vec, len(unit_vecs))\n",
    "\n",
    "        # generate the \"characteristic\" equation for desired eigenvalue\n",
    "        characteristic_eqn = A - desired_eig * np.eye((len_perturbation))\n",
    "\n",
    "        # generate the data matrix for running least squares\n",
    "        Hmat = []  # will be 2*len(indices) X n_chs\n",
    "        for idx, ek in enumerate(unit_vecs):\n",
    "            # determine if to compute row, or column perturbation\n",
    "            if (\n",
    "                perturb_type == constants.PERTURBATIONTYPES.ROW_PERTURBATION.value\n",
    "            ):  # C = inv(A)*ek\n",
    "                Cmat = np.linalg.lstsq(characteristic_eqn, ek, rcond=-1)[0]\n",
    "            elif perturb_type == constants.PERTURBATIONTYPES.COLUMN_PERTURBATION.value:\n",
    "                Cmat = np.linalg.lstsq(characteristic_eqn.T, ek, rcond=-1)[0].T\n",
    "\n",
    "            # extract real and imaginary components to create vector of constraints\n",
    "            Cimat = np.imag(Cmat)\n",
    "            Crmat = np.real(Cmat)\n",
    "\n",
    "            # store these in vector\n",
    "            Hmat.append(Cimat)\n",
    "            Hmat.append(Crmat)\n",
    "        Hmat = np.array(Hmat).squeeze()\n",
    "        # original least squares w/ constraints\n",
    "        if np.imag(desired_eig) == 0:\n",
    "            Bmat = Hmat[1::2, :]\n",
    "            bvec = bvec[1::2]\n",
    "        else:\n",
    "            Bmat = Hmat\n",
    "            bvec = bvec.copy()\n",
    "\n",
    "        # adding orthogonality constraints\n",
    "        # for each orthogonal constraint will make sure the Delta vector(s) are\n",
    "        # 0 at that index. For example, if Delta vector(s) are bi-columnar, then\n",
    "        # orthogonal_constraints of 0 and 50 would make sure the Delta vector\n",
    "        # has value 0 at indices 0 and 50 in both columns\n",
    "        if len(orthogonal_constraints) > 0:\n",
    "            for ekidx in orthogonal_constraints:\n",
    "                ek = _create_standard_basis_vector(\n",
    "                    len_perturbation, ekidx, as_vector=True\n",
    "                )\n",
    "\n",
    "                # orthogonality constraints based on row/column\n",
    "                if (\n",
    "                    self.perturbtype\n",
    "                    == constants.PERTURBATIONTYPES.ROW_PERTURBATION.value\n",
    "                ):\n",
    "                    Orthmat = ek\n",
    "                    Bmat = np.concatenate((Bmat.T, Orthmat), axis=1).T\n",
    "                    bvec = np.hstack((bvec, [0]))\n",
    "                else:\n",
    "                    Orthmat = ek.T\n",
    "                    Bmat = np.concatenate((Bmat, Orthmat), axis=0)\n",
    "                    bvec = np.hstack((bvec, [0])).T\n",
    "\n",
    "        # compute the delta vector that will solve the argmin problem\n",
    "        # print(Bmat.shape)\n",
    "        # print(np.linalg.cond(Bmat))\n",
    "        # print(np.linalg.svd(Bmat))\n",
    "        delvec = np.linalg.pinv(Bmat, rcond=-1).dot(bvec)\n",
    "\n",
    "        # return a 1D vector (real component). Note the\n",
    "        # imaginary component is 0\n",
    "        return delvec.squeeze()\n",
    "\n",
    "    def compute_perturbations_at_indices(\n",
    "        self,\n",
    "        A: np.ndarray,\n",
    "        radius: Union[complex, float, str],\n",
    "        perturbation_strategy: str,\n",
    "        n_perturbations: int = None,\n",
    "        start: int = None,\n",
    "        orthogonal_constraints=None,\n",
    "    ):\n",
    "        \"\"\"Compute single column/row perturbation of an ``A`` matrix.\n",
    "\n",
    "        Note the ``perturb_type`` is set on instantiation of the method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        %(A)s\n",
    "        radius : np.complex\n",
    "            The perturbation radius (``np.abs(radius)``), which may have a real and\n",
    "            imaginary component.\n",
    "        %(perturbation_strategy)s\n",
    "        n_perturbations : int\n",
    "            The number of channels in the original dataset.\n",
    "        %(orthogonal_constraints)s\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        minperturbnorm : np.ndarray\n",
    "            Norms of the perturbation vectors (n_chs, 1).\n",
    "        delvecs_node : np.ndarray\n",
    "            Delta perturbation vectors (n_chs, n_chs). Each row\n",
    "            is a new perturbation vector.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        ``univariate`` perturbation strategy:\n",
    "        Perturbs ``A`` with a delta vector at one column/row indices at a time\n",
    "        to obtain an eigenvalue with value at ``radius``. This will compute\n",
    "        a delta vector and corresponding norms of the delta vectors over\n",
    "        all columns, or rows one at a time.\n",
    "\n",
    "        ``bivariate`` perturbation strategy:\n",
    "        Perturbs ``A`` with a delta vector at two column/row indices at a time\n",
    "        to obtain an eigenvalue with value at ``radius``. This will compute\n",
    "        a delta vector and corresponding norms of the delta vectors over\n",
    "        all columns, or rows one at a time.\n",
    "        \"\"\"\n",
    "        if n_perturbations is None:\n",
    "            n_chs = A[0].shape[0]\n",
    "            if self.perturbation_strategy == \"univariate\":\n",
    "                n_perturbations = n_chs\n",
    "            elif self.perturbation_strategy == \"bivariate\":\n",
    "                n_perturbations = n_chs // 2\n",
    "        n_perturbations = int(n_perturbations)\n",
    "        # initialize array to store all the minimum euclidean norms\n",
    "        minperturbnorm = np.zeros((n_perturbations,))\n",
    "\n",
    "        # initialize NxN array to store the delta vectors with each row being\n",
    "        # a delta vector computed for that specific `ek` unit vector\n",
    "        delvecs_node = np.zeros((n_perturbations, A.shape[0]))\n",
    "\n",
    "\n",
    "        if start is None:\n",
    "            start = 0\n",
    "\n",
    "        for idx, ek in enumerate(range(n_perturbations)):\n",
    "            if perturbation_strategy == \"univariate\":\n",
    "                ek = [ek + start]\n",
    "            elif perturbation_strategy == \"bivariate\":\n",
    "                ek = [ek, ek + (n_perturbations // 2)]  # + start\n",
    "\n",
    "            delta_vec = self._compute_min_norm_delta_vec(\n",
    "                A, radius, ek, orthogonal_constraints=orthogonal_constraints\n",
    "            )\n",
    "\n",
    "            # store the l2 norm of the perturbation vector\n",
    "            min_norm = np.linalg.norm(delta_vec)\n",
    "            # store the minimum norm perturbation and vector\n",
    "            minperturbnorm[idx] = min_norm\n",
    "            # store the vector corresponding to minimum norm perturbation\n",
    "            delvecs_node[idx, :] = delta_vec\n",
    "\n",
    "        self._delta_vec_arr = delvecs_node\n",
    "        self._min_frequency = 0\n",
    "        return minperturbnorm, delvecs_node\n",
    "\n",
    "    def fit(self, A: np.ndarray, **kwargs):\n",
    "        r\"\"\"Compute and setup of the least squares soln.\n",
    "\n",
    "        Assumes, given an A matrix, |lambda| value, and unit vector ek.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        %(A)s\n",
    "        %(orthogonal_constraints)s\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        minperturbnorm : np.ndarray\n",
    "            the l2-norm of the perturbation vector\n",
    "        \"\"\"\n",
    "        radius = self.radius\n",
    "        perturbation_strategy = self.perturbation_strategy\n",
    "\n",
    "        if isinstance(A, np.ndarray):\n",
    "            if A.shape[0] != A.shape[1]:\n",
    "                msg = (\n",
    "                    f\"State matrix must be a square matrix. The state matrix \"\n",
    "                    f\"passed in has shape {A.shape}.\"\n",
    "                )\n",
    "                raise AttributeError(msg)\n",
    "\n",
    "        e_val = np.abs(np.linalg.eigvals(A)).max()\n",
    "        if radius == \"adaptive\":\n",
    "            # get the spectral radii of Amat\n",
    "            radius_ = e_val + 0.1\n",
    "        else:\n",
    "            radius_ = radius\n",
    "        if e_val >= np.abs(radius_):\n",
    "            msg = (\n",
    "                f\"The largest eigenvalue of A has \"\n",
    "                f\"absolute value of {e_val}, so \"\n",
    "                f\"perturbation to {np.abs(radius)} \"\n",
    "                f\"is ill-defined.\"\n",
    "            )\n",
    "            if self.on_error == \"raise\":\n",
    "                raise ValueError(msg)\n",
    "\n",
    "        # make sure A passed in is a list of A matrices\n",
    "        # if passed in a list, assume it is an order p model\n",
    "        if isinstance(A, np.ndarray):\n",
    "            A = [A]\n",
    "\n",
    "        # check that all A's have same shape\n",
    "        assert all([x.shape == A[0].shape for x in A])\n",
    "\n",
    "        if self.perturbation_strategy == \"bivariate\":\n",
    "            assert np.mod(A[0].shape[0], 2) == 0\n",
    "\n",
    "        # check the order of the model imposed\n",
    "        # from state estimation\n",
    "        order = len(A)\n",
    "\n",
    "        # perturbation norms and delta vector list\n",
    "        min_norms = []\n",
    "        delta_vecs_list = []\n",
    "\n",
    "        # run lti perturbation method\n",
    "        for Amat in A:\n",
    "            # This method, only perturbs at :math:`\\omega=0`\n",
    "            minperturbnorm, delta_vecs = self.compute_perturbations_at_indices(\n",
    "                Amat,\n",
    "                radius=radius_,\n",
    "                perturbation_strategy=perturbation_strategy,\n",
    "                **kwargs,\n",
    "            )\n",
    "            min_norms.append(minperturbnorm)\n",
    "            delta_vecs_list.append(delta_vecs)\n",
    "            self._min_norms = min_norms\n",
    "            self._delta_per_node_arr = delta_vecs_list\n",
    "\n",
    "        if order == 1:\n",
    "            return min_norms[0]\n",
    "        return min_norms\n",
    "\n",
    "\n",
    "class ConjugateStructurePerturbation(StructuredPerturbationModel):\n",
    "    \"\"\"Structured rank-1 perturbations that occur over the entire complex disc.\n",
    "\n",
    "    Compared to ``StructuredPerturbationModel``, this model will\n",
    "    perturb over all eigenvalues with absolute value of ``radius``.\n",
    "    The number of points to perturb to will be set by ``mesh_size``.\n",
    "\n",
    "    For example, if ``radius=1.5`` and ``mesh_size=3``, then between\n",
    "    ``r=1.5`` to ``r=-1.5``, there will be 51 points discretized along\n",
    "    the complex disc with radius of 1.5. They will be ``r=1.5j``. We\n",
    "    do not need to perturb at the conjugate pairs, since every\n",
    "    real perturbation vector on a real matrix will produce conjugate\n",
    "    pairs. Therefore we only apply perturbations on the top half\n",
    "    of the complex disc.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    %(radius)s\n",
    "    %(perturb_type)s\n",
    "    mesh_size : int\n",
    "        Number of discrete points (default=51) on the real/imaginary\n",
    "        circle w/ ``np.abs(radius)`` to search over.\n",
    "    %(perturbation_strategy)s\n",
    "    %(n_jobs)s\n",
    "    %(verbose)s\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        radius: float,\n",
    "        perturb_type: str,\n",
    "        mesh_size: int,\n",
    "        perturbation_strategy: str = \"univariate\",\n",
    "        n_jobs: int = 1,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        super(ConjugateStructurePerturbation, self).__init__(\n",
    "            radius=radius,\n",
    "            perturb_type=perturb_type,\n",
    "            perturbation_strategy=perturbation_strategy,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        self._mesh_size = mesh_size\n",
    "\n",
    "    @property\n",
    "    def mesh_size(self):\n",
    "        return self._mesh_size\n",
    "\n",
    "    def _compute_gridsearch_perturbation(self, A, searchnum: int = 51):\n",
    "        r\"\"\"\n",
    "        Generate the minimum norm perturbation model.\n",
    "\n",
    "        This method, does a grid search over combinations of real, or imaginary eigenvalues\n",
    "        that have magnitude of \"radius\". And perturbs at :math:`r = \\sigma + \\omega j`\n",
    "\n",
    "        Reference:\n",
    "        - Sritharan 2014, et al.\n",
    "        - Li 2017, et al.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        %(A)s\n",
    "        searchnum : int\n",
    "            the number of discrete points to split the circle w/ specified radius into\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (minperturbnorm, delvecs_node) : tuple[float, np.ndarray]\n",
    "\n",
    "        minperturbnorm : float\n",
    "            the l2 norm of the perturbation vector\n",
    "        delvecs_node: np.ndarray\n",
    "            [c, c'] is an array of perturbation vectors that achieve minimum norm at each row\n",
    "\n",
    "        \"\"\"\n",
    "        # initialize search parameters to compute minimum norm perturbation\n",
    "        top_wspace = np.linspace(-self.radius, self.radius, num=searchnum)\n",
    "        wspace = np.append(top_wspace, top_wspace[1 : len(top_wspace) - 1], axis=0)\n",
    "        sigma_space = np.sqrt(self.radius ** 2 - top_wspace ** 2)\n",
    "        sigma_space = np.append(\n",
    "            -sigma_space, sigma_space[1 : len(top_wspace) - 1], axis=0\n",
    "        )\n",
    "\n",
    "        # N x N, A matrix\n",
    "        numchans, _ = A.shape\n",
    "\n",
    "        # to store all the euclidean norms of the perturbation vectors\n",
    "        # frequency x channel (F x N)\n",
    "        minnorm_mat = np.zeros((wspace.size, numchans))\n",
    "\n",
    "        # to store all the euclidean norms of the perturbation vectors\n",
    "        # frequency x perturbed index x channel (F x N x N)\n",
    "        delvecs_mat = np.zeros((wspace.size, numchans, numchans), dtype=\"complex\")\n",
    "\n",
    "        # delvecs_list = []\n",
    "\n",
    "        # to store all the frequencies at which perturbation is applied\n",
    "        freqs_mat = np.zeros((wspace.size, 1), dtype=\"complex\")\n",
    "\n",
    "        # compute the min-norm perturbation for omega specified\n",
    "        for i, (sigma, omega) in enumerate(zip(sigma_space, wspace)):\n",
    "            # run lti perturbation method for all contacts\n",
    "            perturbation_results = self._compute_perturbation_on_system(A, sigma, omega)\n",
    "            min_norm_perturb, delvecs_node = perturbation_results\n",
    "\n",
    "            # find index of minimum norm perturbation for channel\n",
    "            minnorm_mat[i, ...] = min_norm_perturb\n",
    "            # delvecs_list.append(delvecs_node)\n",
    "            delvecs_mat[i, ...] = delvecs_node.squeeze()\n",
    "            freqs_mat[i] = sigma + 1j * omega\n",
    "\n",
    "        # store the minimum norms that are achievable for each contact\n",
    "        # -> allows for sensitivity to different frequencies, although majority will be\n",
    "        # sensitive to omega == 0\n",
    "        # determine argmin of the perturbation norms over all frequencies\n",
    "        min_indices = np.argmin(minnorm_mat, axis=0)\n",
    "\n",
    "        # get the minimum perturbation norms, frequencies,\n",
    "        # and delta vectors\n",
    "        min_norm_perturb = np.min(minnorm_mat, axis=0)\n",
    "        min_freqs = sigma_space[min_indices] + 1j * wspace[min_indices]\n",
    "        # delvecs_node = [delvecs_list[i] for i in min_indices]\n",
    "        print(\n",
    "            f\"The minimum indices of {minnorm_mat.shape} \"\n",
    "            f\"has {len(min_indices)} indices.\"\n",
    "        )\n",
    "        print(\"Shape of actual minimum norm perturbation \", min_norm_perturb.shape)\n",
    "        print(\"Shape of minimum norm perturbation freqs \", min_freqs.shape)\n",
    "        print(\"Delta vecs node squeezed: \", delvecs_node.squeeze().shape)\n",
    "        print(delvecs_mat.shape)\n",
    "        delvecs_node = delvecs_mat[min_indices, ...]\n",
    "\n",
    "        self._min_frequency = min_freqs\n",
    "        self._delta_per_node_arr = delvecs_node\n",
    "        self._min_norms = min_norm_perturb\n",
    "        return min_norm_perturb, delvecs_node\n",
    "\n",
    "    def fit(self, A: np.ndarray, orthogonal_constraints: List[int] = None):\n",
    "        \"\"\"Compute perturbation of a square matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        %(A)s\n",
    "        %(orthogonal_constraints)s\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        minperturbnorm : np.ndarray\n",
    "            the l2-norm of the perturbation vector\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445907f4",
   "metadata": {},
   "source": [
    "Mvar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b3c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from mvar_model import SystemIDModel\n",
    "\n",
    "\n",
    "def compute_samplepoints(winsamps, stepsamps, numtimepoints):\n",
    "    # Creates a [n,2] array that holds the sample range of each window that\n",
    "    # is used to index the raw data for a sliding window analysis\n",
    "    samplestarts = np.arange(0, numtimepoints - winsamps + 1.0, stepsamps).astype(int)\n",
    "    sampleends = np.arange(winsamps, numtimepoints + 1, stepsamps).astype(int)\n",
    "\n",
    "    samplepoints = np.append(\n",
    "        samplestarts[:, np.newaxis], sampleends[:, np.newaxis], axis=1\n",
    "    )\n",
    "    return samplepoints\n",
    "\n",
    "\n",
    "def compute_statelds_func(eegwin, **model_params):\n",
    "    mvar_model = SystemIDModel(**model_params)\n",
    "    # 2: compute state transition matrix using mvar model\n",
    "    mvar_model.fit(eegwin)\n",
    "    A_mat = mvar_model.state_array\n",
    "    return A_mat\n",
    "\n",
    "\n",
    "def state_lds_array(\n",
    "    arr: np.ndarray,\n",
    "    winsize: int = 250,\n",
    "    stepsize: int = 125,\n",
    "    l2penalty: float = None,\n",
    "    method_to_use: str = \"pinv\",\n",
    "    n_jobs: int = -1,\n",
    "    memmap: bool = False,\n",
    "    verbose: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute A state matrix from numpy array.\n",
    "\n",
    "    If you have a ``mne.io.Raw`` object, and you want to\n",
    "    run state-lds estimation, then use the ``state_lds_derivative``\n",
    "    function instead.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        ndarray containing EEG data; nxT where n is the number of channels and T is the number of timepoints\n",
    "    sfreq : float\n",
    "        Sampling frequency of the EEG\n",
    "    winsize : int\n",
    "        Window size to sample A matrices in samples\n",
    "    stepsize : int\n",
    "        Increment size, in samples, for sliding window for A matrices\n",
    "    l2penalty : float\n",
    "        penalty for the A matrix estimation - Adds noise when channels are too similar\n",
    "    method_to_use : str\n",
    "        Either 'pinv' or 'hankel'\n",
    "    %(n_jobs)s\n",
    "    %(verbose)s\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A_mats : np.ndarray\n",
    "        nxnxW ndarray where n is the number of channels and W is the number of windows.\n",
    "        \n",
    "    \"\"\"\n",
    "    # data array should be C x T\n",
    "    n_chs, n_signals = arr.shape\n",
    "\n",
    "    # set parameters\n",
    "    model_params = {\n",
    "        \"l2penalty\": l2penalty,\n",
    "        \"method_to_use\": method_to_use,\n",
    "    }\n",
    "\n",
    "    # compute time and sample windows array\n",
    "    sample_points = compute_samplepoints(winsize, stepsize, n_signals)\n",
    "    n_wins = sample_points.shape[0]\n",
    "\n",
    "    # initialize storage container\n",
    "    A_mats = np.zeros((n_chs, n_chs, n_wins))\n",
    "\n",
    "    for idx in range(n_wins):\n",
    "        data = arr[:, sample_points[idx, 0] : sample_points[idx, 1]].T\n",
    "        A_mats[..., idx] = compute_statelds_func(data, **model_params)\n",
    "\n",
    "    return A_mats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65b46b",
   "metadata": {},
   "source": [
    "Mvar model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772bda82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "from sklearn.base import RegressorMixin, MultiOutputMixin\n",
    "from sklearn.linear_model._base import LinearModel\n",
    "\n",
    "from sysid import SIDBase, PostHocMixin\n",
    "\n",
    "SUPPORTED_SYSID_METHODS = [\"pinv\"]\n",
    "\n",
    "\n",
    "def inner_transpose_multicompanion(mat, order):\n",
    "    \"\"\"Transpose inner matrices of a multi-companion matrix.\n",
    "\n",
    "    ``mat`` comprises of a sequence of square matrices,\n",
    "    ``A_1``, ..., ``A_order`` along with corresponding\n",
    "    identity matrices where they should be. This function\n",
    "    will transpose the ``A_1``, ..., ``A_order`` matrices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mat : np.ndarray\n",
    "    order : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mat : np.ndarray\n",
    "        Inner\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    ``mat`` has exactly (n_chs * order, n_chs * order) shape,\n",
    "    so ``n_chs`` can be obtained from the shape of ``mat``.\n",
    "\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    https://core.ac.uk/download/pdf/82792651.pdf\n",
    "    \"\"\"\n",
    "    mat = mat.copy()  # create a copy\n",
    "    n_chs = mat.shape[0] / order\n",
    "\n",
    "    assert np.mod(mat.shape[0], order) == 0\n",
    "\n",
    "    start_col = n_chs * (order - 1)\n",
    "    start_row = 0\n",
    "    for idx in range(order):\n",
    "        grid = np.ix_(\n",
    "            np.arange(start_row, start_row + n_chs, dtype=int),\n",
    "            np.arange(start_col, start_col + n_chs, dtype=int),\n",
    "        )\n",
    "        # get the subset mat matrix\n",
    "        submat = mat[grid]\n",
    "        mat[grid] = submat.T\n",
    "        start_row += n_chs\n",
    "    return mat\n",
    "\n",
    "\n",
    "class SystemIDModel(\n",
    "    SIDBase,\n",
    "    PostHocMixin,\n",
    "    LinearModel,\n",
    "    MultiOutputMixin,\n",
    "    RegressorMixin,\n",
    "):\n",
    "    \"\"\"Multivariate-autoregressive style algorithm used for estimating a linear system.\n",
    "\n",
    "    The model is of the style:\n",
    "\n",
    "        $x(t+1) = Ax(t)$\n",
    "\n",
    "    The algorithm takes in a window of data that is CxT, where C is typically the\n",
    "    number of channels and T is the number of samples for a specific window to generate\n",
    "    the linear system.\n",
    "\n",
    "    It uses either:\n",
    "     1. a dynamic mode decomposition SVD based algorithm,\n",
    "     2. SVD truncated algorithm\n",
    "     3. Pseudoinverse with regularization\n",
    "\n",
    "    to estimate A.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    l2penalty : float\n",
    "        The l2-norm regularization if 'regularize' is turned on. Only used\n",
    "        if ``method_to_use`` is ``'pinv'``.\n",
    "    method_to_use : str\n",
    "        svd as the method to compute A matrix\n",
    "    svd_rank : float\n",
    "        Passed to :func:`eztrack.embed.svd.computeSVD`. If ``None``,\n",
    "        will be the number of channels in the data matrix.\n",
    "        Only used if ``method_to_use`` is ``'svd'``.\n",
    "    tlsq_rank : int\n",
    "        rank truncation computing Total Least Square. Default\n",
    "        is 0, that means no truncation. See [1].\n",
    "    fb : bool\n",
    "        Whether to apply the Forward-Backwards algorithm. See Notes and [2].\n",
    "    order : int\n",
    "        The order of the model to estimate (default=1).\n",
    "    solver : str\n",
    "        Only used if ``method_to_use='sklearn'``. See :func:`sklearn.linear_model.Ridge` parameters.\n",
    "    fit_intercept : bool\n",
    "        Only used if ``method_to_use='sklearn'``. See :func:`sklearn.linear_model.Ridge` parameters.\n",
    "    normalize : bool\n",
    "        Only used if ``method_to_use='sklearn'``. See :func:`sklearn.linear_model.Ridge` parameters.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    When the size of the data is too large (e.g. N > 180, W > 1000), then right now the construction of the csr\n",
    "    matrix scales up. With more efficient indexing, we can perhaps decrease this.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] De-biasing the dynamic mode decomposition for applied Koopman\n",
    "        spectral analysis of noisy datasets. https://arxiv.org/pdf/1703.11004.pdf\n",
    "    .. [2] Characterizing and correcting for the effect of sensor noise in the\n",
    "        dynamic mode decomposition. https://arxiv.org/pdf/1507.02264.pdf\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        l2penalty=0.0,\n",
    "        method_to_use=\"pinv\",\n",
    "        svd_rank=None,\n",
    "        tlsq_rank=0,\n",
    "        fb: bool = False,\n",
    "        solver=\"auto\",\n",
    "        fit_intercept=False,\n",
    "        normalize=False,\n",
    "        order=1,\n",
    "        weighted: bool = False,\n",
    "    ):\n",
    "        super(SystemIDModel, self).__init__()\n",
    "        if method_to_use not in SUPPORTED_SYSID_METHODS:\n",
    "            raise AttributeError(\n",
    "                f\"System ID with {method_to_use} \"\n",
    "                f\"is not supported. Please use \"\n",
    "                f\"one of {SUPPORTED_SYSID_METHODS}.\"\n",
    "            )\n",
    "\n",
    "        self.l2penalty = l2penalty\n",
    "        self.method_to_use = method_to_use\n",
    "        self.svd_rank = svd_rank\n",
    "        self.tlsq_rank = tlsq_rank\n",
    "        self.fb = fb\n",
    "        self.order = order\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.solver = solver\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "\n",
    "    @property\n",
    "    def state_array(self):\n",
    "        \"\"\"The model tensor CxC samples by chan by chan.\"\"\"\n",
    "        return self.state_array_\n",
    "\n",
    "    @property\n",
    "    def eigs(self):\n",
    "        return np.linalg.eigvals(self.state_array)\n",
    "\n",
    "    def _forward_multiply(self, X, t_steps):\n",
    "        # 2D array of initial conditions (C x samples)\n",
    "        n_samples, n_chs = X.shape\n",
    "\n",
    "        X_pred = []\n",
    "        # generate predictions for many initial conditions\n",
    "        for isamp in range(n_samples):\n",
    "            x0 = X[isamp, :]\n",
    "            X_hat = [x0]\n",
    "\n",
    "            # reconstruct over possibly multiple time steps\n",
    "            for it in range(t_steps - 1):\n",
    "                X_hat.append(self.state_array.dot(X_hat[-1]))\n",
    "\n",
    "            # store the predictions\n",
    "            X_hat = np.array(X_hat).T\n",
    "            X_pred.append(X_hat)\n",
    "        # print('inside forward multiply...', np.array(X_pred).shape, n_samples, t_steps)\n",
    "        return np.asarray(X_pred)\n",
    "\n",
    "    def predict(self, X, t_steps: int = 1):\n",
    "        \"\"\"Predict state dynamics from initial conditions as a 2D array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray (n_samples, n_features)\n",
    "            Samples of initial conditions. ``n_features`` should match the\n",
    "            number of channels used in the data to fit model (in ``fit()``).\n",
    "        t_steps : int\n",
    "            The number of time steps to predict. Default = 1.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            True values for X.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(\n",
    "            X, dtype=[np.float64, np.float32], ensure_2d=True, allow_nd=False\n",
    "        )\n",
    "\n",
    "        # forward multiply linear operator\n",
    "        y_pred = self._forward_multiply(X=X, t_steps=t_steps)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y, sample_weight=None, t_steps=1):\n",
    "        \"\"\"Score fitted model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Test samples. For some estimators this may be a\n",
    "            precomputed kernel matrix or a list of generic objects\n",
    "             instead with shape (n_samples, n_samples_fitted),\n",
    "             where n_samples_fitted is the number of samples\n",
    "             used in the fitting for the estimator.\n",
    "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            True values for X.\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights.\n",
    "        t_steps : int\n",
    "            The number of time steps to predict. Default = 1.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            R^2 of self.predict(X) w.r.t. y.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import r2_score\n",
    "\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        X, y = self._validate_data(\n",
    "            X, y, y_numeric=True, dtype=[np.float64, np.float32], multi_output=True\n",
    "        )\n",
    "        if y.ndim == 1:\n",
    "            raise ValueError(\n",
    "                \"y must have at least two dimensions for \"\n",
    "                \"multi-output regression but has only one.\"\n",
    "            )\n",
    "\n",
    "        n_samples, _ = X.shape\n",
    "\n",
    "        # run predictions for possibly multiple initial conditions\n",
    "        y_pred = self.predict(X, t_steps=t_steps)\n",
    "\n",
    "\n",
    "        # get the state prediction and score it\n",
    "        scores = []\n",
    "        for idx in range(n_samples):\n",
    "            X_pred = y_pred[idx, :, :]\n",
    "            X_true = y[idx, :]\n",
    "\n",
    "            score = r2_score(X_true, X_pred, sample_weight=sample_weight)\n",
    "            scores.append(score)\n",
    "        print(\"Finally\")\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Generate adjacency matrix for each window for a certain winsize and stepsize.\n",
    "\n",
    "        Sets the ``state_array`` property, which is the linear operator that\n",
    "        operates over the data matrix ``X``.\n",
    "\n",
    "        Additional hyperparameters are defined in the ``__init__`` function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray (n_samples, n_features)\n",
    "            Samples of multivariate data. ``n_features`` should match the\n",
    "            number of channels used in the data . ``n_samples`` should match\n",
    "            the number of time points in the data window to regress.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This function will fit a linear operator, ``A`` on the data as such::\n",
    "\n",
    "            X[1:, :] = X[:-1, :].dot(A)\n",
    "\n",
    "        representing the equation ``x(t+1) = Ax(t)``.\n",
    "        \"\"\"\n",
    "        # check data\n",
    "        if y is None:\n",
    "            X = self._validate_data(X)\n",
    "        else:\n",
    "            check_kwargs = dict(\n",
    "                dtype=[np.float64, np.float32],\n",
    "                y_numeric=True,\n",
    "                force_all_finite=True,\n",
    "                multi_output=True,\n",
    "            )\n",
    "            X, y = self._validate_data(X, y, **check_kwargs)\n",
    "\n",
    "        # check condition number of the array\n",
    "        cond_num = np.linalg.cond(X)\n",
    "\n",
    "        # 1. determine shape of the window of data\n",
    "        n_times, n_chs = X.shape\n",
    "\n",
    "        # determine svd_rank if it is None\n",
    "        if self.svd_rank is None:\n",
    "            svd_rank = min(n_chs, n_times)\n",
    "        else:\n",
    "            svd_rank = self.svd_rank\n",
    "\n",
    "        # allow for multiple order matrix estimation\n",
    "        # first make sure data matrix is 2D\n",
    "        # also swap channels and times to make it backwards compatible\n",
    "        # with our change to sklearn API\n",
    "        self._snapshots, self._snapshots_shape = self._col_major_2darray(X.T)\n",
    "\n",
    "        # create large snapshot with time-lags of order specified by\n",
    "        # ``order`` value\n",
    "        snaps = self._construct_snapshots(\n",
    "            self._snapshots, order=self.order, n_times=n_times\n",
    "        )\n",
    "        X, Y = snaps[:, :-1], snaps[:, 1:]\n",
    "\n",
    "        # compute total-least squares if necessary\n",
    "        X, Y = self._compute_tlsq(X, Y, self.tlsq_rank)\n",
    "\n",
    "        # compute inverse of variance weights\n",
    "        if self.weighted:\n",
    "            # compute along the samples dimension\n",
    "            sample_var = np.var(X, axis=1)\n",
    "            sample_weight = 1.0 / sample_var\n",
    "\n",
    "            eps = 1e-8\n",
    "            if np.any(sample_var < eps):\n",
    "                sample_weight = None\n",
    "        else:\n",
    "            sample_weight = None\n",
    "\n",
    "        # compute the forward linear operator from X -> Y\n",
    "        if self.method_to_use == \"pinv\":\n",
    "            A = self._pinv_solve(X, Y, self.l2penalty, solver=\"svd\", backend=\"numpy\")\n",
    "\n",
    "        # if using forward-backwards algorithm, then\n",
    "        # compute the backwards operator and combine the two\n",
    "        if self.fb:\n",
    "            # compute backward linear operator\n",
    "            bA = self._pinv_solve(Y, X, l2_penalty=self.l2penalty)\n",
    "            A = sqrtm(A.dot(np.linalg.inv(bA)))\n",
    "\n",
    "        self.state_array_ = A\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5f51c",
   "metadata": {},
   "source": [
    "Fragility file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "679b460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def _get_tempfilename(x):\n",
    "    \"\"\"Hardcoded temporary file name for storing temporary results.\"\"\"\n",
    "    return \"temp_{}.npz\".format(x)\n",
    "\n",
    "\n",
    "def _compute_fragility_func(\n",
    "    shared_mvarmodel: SystemIDModel,\n",
    "    shared_pertmodel: StructuredPerturbationModel,\n",
    "    raw_data: np.ndarray,\n",
    "    samplepoints: np.ndarray,\n",
    "    tempdir: Union[str, Path],\n",
    "    win: int,\n",
    "    # type_indices: Dict[str, List],\n",
    "    **kwargs,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:  # pragma: no cover\n",
    "    \"\"\"Parallel computation of network.\n",
    "\n",
    "    Computes for a single window, the A matrix and\n",
    "    corresponding perturbation vector with minimum 2-norm.\n",
    "\n",
    "    Note: The `raw_data` and `win` parameters can be replaced with `raw`\n",
    "        of the MNE data structure in the future if mem-mapping is desired.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shared_mvarmodel : eztrack.network.MvarModel\n",
    "        The model used to compute the A state matrix for a window of time.\n",
    "    shared_pertmodel : eztrack.network.MinNormPerturbModel\n",
    "        The model used to compute the vector of perturbation norms for each\n",
    "        column/row perturbed in the A matrix.\n",
    "    raw_data : np.ndarray\n",
    "        The raw EEG data C x T (channels by time)\n",
    "    samplepoints : np.ndarray\n",
    "    tempdir : str | pathlib.Path\n",
    "    win : int\n",
    "        The specific window that `raw_data` was obtained from within\n",
    "        the larger EEG snapshot.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pert_mat : np.ndarray\n",
    "    A_mat : np.ndarray\n",
    "    delta_vecs : np.ndarray\n",
    "    \"\"\"\n",
    "    # Avoid circular import\n",
    "\n",
    "    # 1: fill matrix of all channels' next EEG data over window\n",
    "    win_begin = samplepoints[win, 0]\n",
    "    win_end = samplepoints[win, 1] + 1\n",
    "    eegwin = raw_data[:, win_begin:win_end].T  # samples x channels\n",
    "\n",
    "    # 2: compute state transition matrix using mvar model\n",
    "    shared_mvarmodel.fit(eegwin)\n",
    "    A_mat = shared_mvarmodel.state_array\n",
    "\n",
    "    # in higher-order models, one needs to apply perturbations\n",
    "    # to a multi-companion matrix\n",
    "    if shared_mvarmodel.order > 1 and shared_pertmodel.perturb_type == \"C\":\n",
    "        # flip matrix\n",
    "        to_perturb_A = inner_transpose_multicompanion(\n",
    "            A_mat.copy().T, order=shared_mvarmodel.order\n",
    "        )\n",
    "    else:\n",
    "        to_perturb_A = A_mat.copy()\n",
    "\n",
    "    # 3: compute perturbation model\n",
    "    pert_mat = shared_pertmodel.fit(to_perturb_A, **kwargs)\n",
    "    delta_vecs = shared_pertmodel.minimum_delta_vectors\n",
    "\n",
    "    if tempdir is not None:\n",
    "        # save adjacency matrix\n",
    "        tempfilename = os.path.join(tempdir, _get_tempfilename(win))\n",
    "        try:\n",
    "            np.savez(tempfilename, A=A_mat, pertmat=pert_mat, delta_vecs=delta_vecs)\n",
    "        except BaseException as e:\n",
    "            return (None, None, None)\n",
    "    return pert_mat, A_mat, delta_vecs\n",
    "\n",
    "\n",
    "def _compute_perturbation_func(\n",
    "    A_mat: np.ndarray, radius: float, perturb_type: str, order: int, **kwargs\n",
    ") -> Tuple[np.ndarray, np.ndarray]:  # pragma: no cover\n",
    "    \"\"\"Parallel computation of network.\n",
    "\n",
    "    Computes for a single window, the A matrix and\n",
    "    corresponding perturbation vector with minimum 2-norm.\n",
    "\n",
    "    Note: The `raw_data` and `win` parameters can be replaced with `raw`\n",
    "        of the MNE data structure in the future if mem-mapping is desired.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    radius : float\n",
    "        The radius to make perturbed eigenvalue go to.\n",
    "    perturb_type : str\n",
    "        The type of structured perturbation to take. Either ``'R'``,\n",
    "        or ``'C'``.\n",
    "    A_mat : np.ndarray\n",
    "        The state matrix (C x C) for x(t+1) = Ax(t).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pert_mat : np.ndarray\n",
    "    delta_vecs : np.ndarray\n",
    "    \"\"\"\n",
    "    # Avoid circular import\n",
    "\n",
    "    pertmodel_kwargs = {\n",
    "        \"radius\": radius,\n",
    "        \"perturb_type\": perturb_type,\n",
    "    }\n",
    "    pertmodel_kwargs.update(kwargs)\n",
    "    pert_model = StructuredPerturbationModel(**pertmodel_kwargs)\n",
    "\n",
    "    # which column/row to start perturbations at\n",
    "    n_perturbations = A_mat.shape[0] / order\n",
    "    if order > 1:\n",
    "        start_idx = int((order - 1) * n_perturbations)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "\n",
    "    # in higher-order models, one needs to apply perturbations\n",
    "    # to a multi-companion matrix\n",
    "    if order > 1 and perturb_type == \"C\":\n",
    "        # flip matrix\n",
    "        to_perturb_A = inner_transpose_multicompanion(A_mat.copy().T, order=order)\n",
    "    else:\n",
    "        to_perturb_A = A_mat.copy()\n",
    "\n",
    "    # 3: compute perturbation model\n",
    "    pert_mat = pert_model.fit(\n",
    "        to_perturb_A, start=start_idx, n_perturbations=n_perturbations\n",
    "    )\n",
    "    delta_vecs = pert_model.minimum_delta_vectors\n",
    "\n",
    "    return pert_mat, delta_vecs\n",
    "\n",
    "\n",
    "def state_perturbation_array(\n",
    "    arr: np.ndarray,\n",
    "    order: int = 1,\n",
    "    radius: float = 1.5,\n",
    "    perturb_type: str = \"C\",\n",
    "):\n",
    "    \"\"\"Compute network on state-lds dataset.\n",
    "\n",
    "    This function operates on a numpy array that is assumed\n",
    "    to be structured channels X channels X time.\n",
    "\n",
    "    If you have a ``StateLDSDerivative`` object, then\n",
    "    use ``state_perturbation_derivative`` function\n",
    "    instead.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        The state matrices in the form (C x C x T)\n",
    "    radius : float\n",
    "        The radius to make perturbed eigenvalue go to.\n",
    "    perturb_type : str\n",
    "        The type of structured perturbation to take. Either ``'R'``,\n",
    "        or ``'C'``.\n",
    "    %(verbose)s\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pert_mats : np.ndarray\n",
    "    delta_vecs_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    if arr.shape[0] != arr.shape[1]:\n",
    "        raise RuntimeError(\n",
    "            f\"State matrix data must be in a \" f\"channel X channel X time shape.\"\n",
    "        )\n",
    "    n_chs, n_chs_, n_wins = arr.shape\n",
    "\n",
    "    model_params = {\n",
    "        \"order\": order,\n",
    "        \"radius\": radius,\n",
    "        \"perturb_type\": perturb_type,\n",
    "    }\n",
    "\n",
    "    if order > 1:\n",
    "        n_perturbations = int(arr.shape[0] / order)\n",
    "    else:\n",
    "        n_perturbations = n_chs\n",
    "\n",
    "    # initialize numpy arrays to return results\n",
    "    pert_mats = np.zeros((n_perturbations, n_wins))\n",
    "    delta_vecs_arr = np.zeros((n_perturbations, n_chs, n_wins), dtype=complex)\n",
    "\n",
    "    for idx in range(n_wins):\n",
    "        pert_mat, delta_vecs = _compute_perturbation_func(arr[..., idx], **model_params)\n",
    "        pert_mats[:, idx] = pert_mat\n",
    "        delta_vecs_arr[..., idx] = delta_vecs\n",
    "\n",
    "    return pert_mats, delta_vecs_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d8ec6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import detrend\n",
    "import mne\n",
    "def outlier_repeat(data: np.ndarray, sd: float, rounds: int = np.inf,\n",
    "                   axis: int = 0) -> tuple[tuple[int, int]]:\n",
    "    \"\"\" Remove outliers from data and repeat until no outliers are left.\n",
    "\n",
    "    This function removes outliers from data and repeats until no outliers are\n",
    "    left. Outliers are defined as any data point that is more than sd standard\n",
    "    deviations from the mean. The function returns a tuple of tuples containing\n",
    "    the index of the outlier and the round in which it was removed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Data to remove outliers from.\n",
    "    sd : float\n",
    "        Number of standard deviations from the mean to consider an outlier.\n",
    "    rounds : int\n",
    "        Number of times to repeat outlier removal. If None, the function will\n",
    "        repeat until no outliers are left.\n",
    "    axis : int\n",
    "        Axis of data to remove outliers from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[tuple[int, int]]\n",
    "        Tuple of tuples containing the index of the outlier and the round in\n",
    "        which it was removed.\"\"\"\n",
    "    inds = list(range(data.shape[axis]))\n",
    "\n",
    "    # Square the data and set zeros to small positive number\n",
    "    R2 = np.square(data)\n",
    "    R2[np.where(R2 == 0)] = 1e-9\n",
    "\n",
    "    # find all axes that are not channels (example: time, trials)\n",
    "    axes = tuple(i for i in range(data.ndim) if not i == axis)\n",
    "\n",
    "    # Initialize stats loop\n",
    "    sig = np.std(R2, axes)  # take standard deviation of each channel\n",
    "    cutoff = (sd * np.std(sig)) + np.mean(sig)  # outlier cutoff\n",
    "    i = 1\n",
    "\n",
    "    # remove bad channels and re-calculate variance until no outliers are left\n",
    "    while np.any(np.where(sig > cutoff)) and i <= rounds:\n",
    "\n",
    "        # Pop out names to bads output using comprehension list\n",
    "        for j, out in enumerate(np.where(sig > cutoff)[0]):\n",
    "            yield inds.pop(out - j), i\n",
    "\n",
    "        # re-calculate per channel variance\n",
    "        R2 = R2[..., np.where(sig < cutoff)[0], :]\n",
    "        sig = np.std(R2, axes)\n",
    "        cutoff = (sd * np.std(sig)) + np.mean(sig)\n",
    "        i += 1\n",
    "\n",
    "def channel_outlier_marker(input_raw, outlier_sd=3,\n",
    "                           max_rounds=np.inf, axis=0,\n",
    "                            verbose=True\n",
    "                           ) -> list[str]:\n",
    "    \"\"\"Identify bad channels by variance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_raw : Signal\n",
    "        Raw data to be analyzed.\n",
    "    outlier_sd : int, optional\n",
    "        Number of standard deviations above the mean to be considered an\n",
    "        outlier, by default 3\n",
    "    max_rounds : int, optional\n",
    "        Maximum number of variance estimations, by default runs until no\n",
    "        more bad channels are found.\n",
    "    axis : int, optional\n",
    "        Axis to calculate variance over, by default 0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        List of bad channel names.\n",
    "    \"\"\" \n",
    "\n",
    "    names = input_raw.copy().pick('data').ch_names\n",
    "    data = detrend(input_raw.get_data('data'))  # channels X time\n",
    "    bads = []  # output for bad channel names\n",
    "    desc = []  # output for bad channel descriptions\n",
    "\n",
    "    # Pop out names to bads output using comprehension list\n",
    "    for ind, i in outlier_repeat(data, outlier_sd, max_rounds, axis):\n",
    "        bads.append(names[ind])\n",
    "        desc.append(f'outlier round {i} more than {outlier_sd} SDs above mean')\n",
    "        # log channels excluded per round\n",
    "        if verbose:\n",
    "            mne.utils.logger.info(f'outlier round {i} channels: {bads}')\n",
    "\n",
    "\n",
    "    return bads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a395669c",
   "metadata": {},
   "source": [
    "## Preprocessing and implementing fragility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6bb807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw):\n",
    "    # Drop bad channels\n",
    "    raw.drop_channels(raw.info['bads'])\n",
    "    raw.load_data()\n",
    "    # Notch filter at 60 Hz to remove line noise\n",
    "    notch_filtered = raw.notch_filter(60, notch_widths=2)\n",
    "    # Extract bandapss 0.5 to 300 Hz using fourth order Butterworth filter\n",
    "    final_raw = notch_filtered.filter(0.5, 300, \n",
    "                                    method='iir')\n",
    "\n",
    "    outliers = channel_outlier_marker(final_raw, 3, 2)\n",
    "    final_raw.drop_channels(outliers)\n",
    "    # Set common average reference \n",
    "    final_raw.set_eeg_reference()\n",
    "\n",
    "    return final_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da52e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.io import read_raw_edf\n",
    "from mne.filter import notch_filter, filter_data\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def run_fragility(raw, model_params, events=None, tmin=-10, tmax=10):\n",
    "    '''\n",
    "    Runs fragility code on given raw file. \n",
    "\n",
    "    -------------------------------------\n",
    "    Input parameters:\n",
    "    raw: MNE raw file to run fragility code on\n",
    "    model_params: dictionary, we are defaulting to prior lab's normal values\n",
    "    events: List of strings corresponding to annotations to run; if no event is specified\n",
    "    it will run fragility code on entire file \n",
    "    tmin: start time to run code with respect to annotation, defaults to 0 (only needed if \n",
    "    specific events are specficied)\n",
    "    tmax: end time in seconds to run code, defaults to 10 (only needed if specific events\n",
    "    are specified)\n",
    "\n",
    "    returns:\n",
    "    pert_mats: pertubation matrices corresponding to fragility scores with dimensions\n",
    "    channels, time points. Saved as a dictionary with keys corresponding to input event strings.\n",
    "    delta_vecs_arr: array of delta vectors used to make pertubation array\n",
    "    '''\n",
    "    \n",
    "    # raw.info['bads'] = channel_outlier_marker(raw, 3, 2)\n",
    "    sfreq = raw.info[\"sfreq\"]\n",
    "    \n",
    "    # Remove non-SEEG channels\n",
    "    bad_words = ['C', 'TRIG', 'OSAT', 'PR', 'Pleth', 'EKG']\n",
    "    for ch in raw.ch_names:\n",
    "        if any(word in ch for word in bad_words):\n",
    "            raw.info['bads'].append(ch)\n",
    "        # Remove EEG channels based off length of string (EEG channels are no longer than 3 chars)\n",
    "        if len(ch) < 4:\n",
    "            raw.info['bads'].append(ch)\n",
    "\n",
    "\n",
    "    winsize_sec = model_params.get(\"winsize_sec\")\n",
    "    stepsize_sec = model_params.get(\"stepsize_sec\")\n",
    "    l2penalty = model_params.get(\"l2penalty\")\n",
    "\n",
    "    # Calculate how many time points are in each window\n",
    "    winsize_samps = int(round(winsize_sec * sfreq))\n",
    "    stepsize_samps = int(round(stepsize_sec * sfreq))\n",
    "\n",
    "    # Do preprocessing on raw data\n",
    "    final_raw = preprocess(raw)\n",
    "  \n",
    "    # Initialize dictionaries containing fragility metrics \n",
    "    pert_mats = {}\n",
    "    delta_vecs_arrs = {}\n",
    "\n",
    "    # Use whole file if events is None\n",
    "    if events == None:\n",
    "        epoch_data = final_raw.get_data()\n",
    "        A_mats = state_lds_array(epoch_data, winsize=winsize_samps, stepsize=stepsize_samps, l2penalty=l2penalty)\n",
    "        pert_mats['whole'], delta_vecs_arrs['whole'] = state_perturbation_array(A_mats)\n",
    "\n",
    "    # If events is specfified, make epochs from given annotations\n",
    "    else:\n",
    "        _, event_dict = mne.events_from_annotations(final_raw)\n",
    "        events_of_interest = {}\n",
    "        for ee in events:\n",
    "            events_of_interest[np.str_(ee)] = event_dict[np.str_(ee)]\n",
    "\n",
    "        epochs = mne.Epochs(final_raw, tmin=tmin, tmax=tmax, event_id=events_of_interest, baseline=(0, 0))\n",
    "\n",
    "\n",
    "        for ee in events:\n",
    "            print(f'Running on epoch labeled {ee}')\n",
    "            epoch_data = epochs[ee].get_data().squeeze()\n",
    "            A_mats = state_lds_array(epoch_data, winsize=winsize_samps, stepsize=stepsize_samps, l2penalty=l2penalty)\n",
    "            pert_mats[ee], delta_vecs_arrs[ee] = state_perturbation_array(A_mats)\n",
    "    \n",
    "    return pert_mats, delta_vecs_arrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c450d9c0",
   "metadata": {},
   "source": [
    "## Run fragility on input file\n",
    "Repalce edf_fpath with desired file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "695a646c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /Users/dsexton/Downloads/fragility_code/DATA/MC01_1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Events in raw: ['NFIED1' 'Spike' 'X']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/q2rhnx5x22j0x58jmr65fczc0000gn/T/ipykernel_37413/2099679415.py:8: RuntimeWarning: Omitted 11 annotation(s) that were outside data range.\n",
      "  raw = read_raw_edf(edf_fpath)\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    \"winsize_sec\": 0.5,\n",
    "    \"stepsize_sec\": 0.5,\n",
    "    \"l2penalty\": None\n",
    "}\n",
    "\n",
    "edf_fpath = Path.home() / 'Downloads' / 'fragility_code' / 'DATA' / 'MC01_1.edf'\n",
    "raw = read_raw_edf(edf_fpath)\n",
    "print(f'Events in raw: {raw.annotations.description}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd612e",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "Takes values from pert_mats output from run_fragility and plots heatmap for each channel corresponding to fragility scores using plot_heatmap function. \n",
    "\n",
    "The second technique for analysis involves specifying a fragility score threshold (between 0 and 1) and counting how many times each channel crosses this threshold. \n",
    "Uses plot_threshold function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5ffe813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "def plot_fragility(raw, pert_mats, output_file, threshold='auto'):\n",
    "    ch_num = len(raw.ch_names)\n",
    "    ch_inds = [0, ch_num // 4, ch_num // 2, 3 * ch_num // 4, ch_num]\n",
    "    \n",
    "    # Loop through each event in pert_mat and plot heatmap and thresholds \n",
    "    for kk in pert_mats.keys():\n",
    "        fig, axes = plt.subplots(2, 2)\n",
    "        fig.set_size_inches([16, 12])\n",
    "        cax = fig.add_axes([0.92, 0.1, 0.02, 0.8])\n",
    "        fig.suptitle(kk, size=26)\n",
    "        for ii, ax in enumerate(axes.flatten()):\n",
    "            ch_slice = slice(ch_inds[ii], ch_inds[ii+1])\n",
    "            sns.heatmap(pert_mats[kk][ch_slice, :], vmax=1, cmap='viridis', ax=ax, yticklabels=raw.ch_names[ch_slice], cbar=ii == 0, cbar_ax=None if ii != 0 else cax)\n",
    "            ax.tick_params(axis='y', labelsize=6)\n",
    "        fig.savefig(f'{output_file}_{kk}_heatmap.png')\n",
    "\n",
    "        \n",
    "\n",
    "        # Plot thresholds \n",
    "        if threshold == 'auto':\n",
    "            threshold = pert_mats[kk].mean() + pert_mats[kk].std()\n",
    "        threshold_mat = np.where(pert_mats[kk] >= threshold, 1, 0)\n",
    "        \n",
    "        # Count number of times fragility scores crosses threshold for each channel\n",
    "        ch_sum = threshold_mat.sum(axis=1)\n",
    "\n",
    "        # Create dataframe with threshold values, sort, and remove channels with no threshold crossing        \n",
    "        threshold_df = pd.DataFrame({'Count': ch_sum, 'Name': raw.ch_names}).sort_values(by=['Count'])\n",
    "        final_df = threshold_df[threshold_df['Count'] != 0]\n",
    "\n",
    "        fig, axes = plt.subplots()\n",
    "        fig.set_size_inches([16, 12])\n",
    "        fig.suptitle(kk, size=26)\n",
    "\n",
    "        sns.barplot(data=final_df, x='Name', y='Count', ax=axes)\n",
    "        axes.tick_params(axis='x', labelsize=10, rotation=90)\n",
    "        fig.savefig(f'{output_file}_{kk}_thresholds.png')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19a91650",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fragility(raw, pert_mats, output_file='MC01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e65b59",
   "metadata": {},
   "source": [
    "### Look at accuracy of model (A matrix) fitting of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3eb094c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /Users/dsexton/Downloads/fragility_code/DATA/MC01_1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/q2rhnx5x22j0x58jmr65fczc0000gn/T/ipykernel_37413/773672197.py:8: RuntimeWarning: Omitted 11 annotation(s) that were outside data range.\n",
      "  raw = read_raw_edf(edf_fpath)\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    \"winsize_sec\": 0.5,\n",
    "    \"stepsize_sec\": 0.5,\n",
    "    \"l2penalty\": None\n",
    "}\n",
    "\n",
    "edf_fpath = Path.home() / 'Downloads' / 'fragility_code' / 'DATA' / 'MC01_1.edf'\n",
    "raw = read_raw_edf(edf_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a0da5d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 322111  =      0.000 ...   157.281 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 58 - 62 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 58.50\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 58.25 Hz)\n",
      "- Upper passband edge: 61.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 61.75 Hz)\n",
      "- Filter length: 13517 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 3e+02 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 0.50, 300.00 Hz: -6.02, -6.02 dB\n",
      "\n",
      "outlier round 1 channels: ['ROAM8']\n",
      "outlier round 2 channels: ['ROAM8', 'RLS5']\n",
      "outlier round 2 channels: ['ROAM8', 'RLS5', 'RLS6']\n",
      "outlier round 2 channels: ['ROAM8', 'RLS5', 'RLS6', 'ROAM7']\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n"
     ]
    }
   ],
   "source": [
    "sfreq = raw.info[\"sfreq\"]\n",
    "\n",
    "# Remove non-SEEG channels\n",
    "bad_words = ['C', 'TRIG', 'OSAT', 'PR', 'Pleth', 'EKG']\n",
    "for ch in raw.ch_names:\n",
    "    if any(word in ch for word in bad_words):\n",
    "        raw.info['bads'].append(ch)\n",
    "    # Remove EEG channels based off length of string (EEG channels are no longer than 3 chars)\n",
    "    if len(ch) < 4:\n",
    "        raw.info['bads'].append(ch)\n",
    "\n",
    "\n",
    "winsize_sec = model_params.get(\"winsize_sec\")\n",
    "stepsize_sec = model_params.get(\"stepsize_sec\")\n",
    "l2penalty = model_params.get(\"l2penalty\")\n",
    "\n",
    "# Calculate how many time points are in each window\n",
    "winsize_samps = int(round(winsize_sec * sfreq))\n",
    "stepsize_samps = int(round(stepsize_sec * sfreq))\n",
    "\n",
    "# Do preprocessing on raw data\n",
    "final_raw = preprocess(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4b344084",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_raw_cropped = final_raw.crop(tmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7abb35c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ieeg_pipe/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "winsize_sec = 0.5\n",
    "stepsize_sec = 0.5\n",
    "final_raw_data = final_raw_cropped.get_data()\n",
    "winsize_samps = int(round(winsize_sec * final_raw_cropped.info['sfreq']))\n",
    "stepsize_samps = int(round(stepsize_sec * final_raw_cropped.info['sfreq']))\n",
    "A_mats = state_lds_array(final_raw_data, winsize=winsize_samps, stepsize=stepsize_samps, l2penalty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e0297be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147, 147, 20)\n",
      "(20, 1024, 147)\n"
     ]
    }
   ],
   "source": [
    "print(A_mats.shape)\n",
    "\n",
    "# Initialize x_hat list to collect x_hat values across windows\n",
    "x_hat_cross_windows = []\n",
    "# Loop across windows \n",
    "for win in range(20):\n",
    "    x_hat_lst = []\n",
    "    # Initialize x_hat(0) to initial EEG value\n",
    "    x_hat_lst.append(final_raw_data[:, win*winsize_samps])\n",
    "    \n",
    "    # Loop through each sample and calcualte x_hat using A matrix\n",
    "    for ii in range(1, winsize_samps):\n",
    "        x_hat_lst.append(A_mats[:, :, win] @ x_hat_lst[ii-1])\n",
    "    x_hat = np.stack(x_hat_lst)\n",
    "    x_hat_cross_windows.append(x_hat)\n",
    "\n",
    "# Create array of all x_hat values\n",
    "x_hat_all = np.stack(x_hat_cross_windows)\n",
    "print(x_hat_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e1b02641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corr_dict = {}\n",
    "corr_dict['Ch'] =final_raw_cropped.ch_names\n",
    "for win in range(20):\n",
    "    x_true = final_raw_data[:, winsize_samps*win:winsize_samps*(win+1)]\n",
    "    corr_list = []\n",
    "    for ch in range(len(final_raw_cropped.ch_names)):\n",
    "        corr_list.append(np.corrcoef(x_true[ch, :], x_hat_all[win, :, ch])[0, 1])\n",
    "    corr_dict[f'Window {win}'] = corr_list\n",
    "\n",
    "corr_df = pd.DataFrame(corr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ed400419",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.melt(corr_df, id_vars=['Ch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9519835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "fig.set_size_inches([16, 12])\n",
    "fig.suptitle(r'Correlation between $x$ and $\\hat{x}$', size=26)\n",
    "\n",
    "\n",
    "sns.boxplot(data=final_df, x='Ch', y='value', ax=axes)\n",
    "axes.tick_params(axis='x', labelsize=10, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348c30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg_pipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
